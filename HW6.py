# PART 1
import pandas as pd
import numpy as np

from sklearn.model_selection import cross_val_predict
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import r2_score

import matplotlib.pyplot as plt

df = pd.read_csv("https://courses.edx.org/asset-v1:HarvardX+PH526x+2T2019+type@asset+block@movie_data.csv", index_col=0)

pd.set_option("display.max_columns", None)
df.head()

# Exercise 2
# Create a new column in df called profitable, defined as 1 if the movie revenue (revenue) is greater than the movie budget (budget), and 0 otherwise.
# Next, define and store the outcomes we will use for regression and classification. Define regression_target as the string 'revenue'. Define classification_target as the string 'profitable'.
df.loc[df["revenue"] > df["budget"], "profitable"] = 1
df.loc[df["revenue"] <= df["budget"], "profitable"] = 0
regression_target = "revenue"
classification_target = "profitable"
#How many movies in this dataset are defined as profitable (value 1)?
df.profitable.value_counts()

# Exercise 3
# Use df.replace() to replace any cells with type np.inf or -np.inf with np.nan.
# Drop all rows with any np.nan values in that row using df.dropna(). Do any further arguments need to be specified in this function to remove rows with any such values?
#How many movies are left in the dataset after dropping any rows with infinite or missing values?

df = df.replace([np.inf, -np.inf], np.nan)
df = df.dropna()
df.shape

# Exercise 4
# Determine all the genres in the genre column. Make sure to use the strip() function on each genre to remove trailing characters.
# Next, include each listed genre as a new column in the dataframe. Each element of these genre columns should be 1 if the movie belongs to that particular genre, and 0 otherwise. Keep in mind that a movie may belong to several genres at once.
# Call df[genres].head() to view your results.
# How many genres of movies are in this dataset?
list_genres = df.genres.apply(lambda x: x.split(","))
genres = []
for row in list_genres:
    row = [genre.strip() for genre in row]
    for genre in row:
        if genre not in genres:
            genres.append(genre)
for genre in genres:
    df[genre] = df['genres'].str.contains(genre).astype(int)
len(genres)

# Exercise 5
# Call plt.show() to observe the plot generated by the code given below. Which of the covariates and/or outcomes are correlated with each other?
# Call skew() on the columns outcomes_and_continuous_covariates in df. Is the skew above 1 for any of these variables?
continuous_covariates = ['budget', 'popularity', 'runtime', 'vote_count', 'vote_average']
outcomes_and_continuous_covariates = continuous_covariates + [regression_target, classification_target]
plotting_variables = ['budget', 'popularity', regression_target]

axes = pd.plotting.scatter_matrix(df[plotting_variables], alpha=0.15, color=(0,0,0), hist_kwds={"color":(0,0,0)}, facecolor=(1,0,0))
plt.show()

print(df[outcomes_and_continuous_covariates].skew())

# Exercise 6
# It appears that the variables budget, popularity, runtime, vote_count, and revenue are all right-skewed. In Exercise 6, we will transform these variables to eliminate this skewness. Specifically, we will use the np.log10() method. Because some of these variable values are exactly 0, we will add a small positive value to each to ensure it is defined; this is necessary because log(0) is negative infinity.
# For each above-mentioned variable in df, transform value x into np.log10(1+x).
# What is the new value of skew() for the covariate runtime? Please provide the answer to 3 decimal points.
variables = ['budget', 'popularity', 'runtime', 'vote_count', 'revenue']
for variable in variables:
    df[variable] = df[variable].apply(lambda x: np.log10(1+x))
print(df[outcomes_and_continuous_covariates].skew())

# Exercise 7
# Use to_csv() to save the df object as movies_clean.csv.
df.to_csv("movies_clean.csv")

# PART 2
import pandas as pd
import numpy as np

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score
from sklearn.metrics import r2_score

import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings("ignore")

# EDIT THIS CODE TO LOAD THE SAVED DF FROM PART 1
df = pd.read_csv("movies_clean.csv")

# Exercise 1
# Instantiate LinearRegression(), LogisticRegression(), RandomForestRegressor(), and RandomForestClassifier() objects, and assign them to linear_regression, logistic_regression, forest_regression, and forest_classifier, respectively.
# For the random forests models, specify max_depth=4 and random_state=0.
regression_target = 'revenue'
classification_target = 'profitable'
all_covariates = ['budget', 'popularity', 'runtime', 'vote_count', 'vote_average', 'Action', 'Adventure', 'Fantasy', 
                  'Science Fiction', 'Crime', 'Drama', 'Thriller', 'Animation', 'Family', 'Western', 'Comedy', 'Romance', 
                  'Horror', 'Mystery', 'War', 'History', 'Music', 'Documentary', 'TV Movie', 'Foreign']

regression_outcome = df[regression_target]
classification_outcome = df[classification_target]
covariates = df[all_covariates]

# Instantiate all regression models and classifiers.
linear_regression = LinearRegression()
logistic_regression = LogisticRegression()
forest_regression = RandomForestRegressor(max_depth=4, random_state=0)
forest_classifier = RandomForestClassifier(max_depth=4, random_state=0)
    
# Exercise 2
# Define a function called correlation with arguments estimator, X, and y. The function should compute the correlation between the observed outcome y and the outcome predicted by the model.
       # To obtain predictions, the function should first use the fit method of estimator and then use the predict method from the fitted object.
       #  The function should return the first argument from r2_score comparing predictions and y.
# Define a function called accuracy with the same arguments and code, substituting accuracy_score for r2_score.
def correlation(estimator, X, y):
    predictions = estimator.fit(X, y).predict(X)
    return r2_score(y, predictions)

def accuracy(estimator, X, y):
    predictions = estimator.fit(X, y).predict(X)
    return accuracy_score(y, predictions)

# Exercise 3
# Call cross_val_score using linear_regression and forest_regression as models. Store the output as linear_regression_scores and forest_regression_scores, respectively.
 # Set the parameters cv=10 to use 10-fold cross-validation and scoring=correlation to use the correlation function defined in Exercise 2.
# Plotting code has been provided to compare the performance of the two models. Use plt.show() to plot the correlation between actual and predicted revenue for each cross-validation fold using the linear and random forest regression models.
# Consider which of the two models exhibits a better fit.
linear_regression_scores = cross_val_score(linear_regression, covariates, regression_outcome, cv=10, scoring=correlation)
forest_regression_scores = cross_val_score(forest_regression, covariates, regression_outcome, cv=10, scoring=correlation)

# Plot Results
plt.axes().set_aspect('equal', 'box')
plt.scatter(linear_regression_scores, forest_regression_scores)
plt.plot((0, 1), (0, 1), 'k-')

plt.xlim(0, 1)
plt.ylim(0, 1)
plt.xlabel("Linear Regression Score")
plt.ylabel("Forest Regression Score")

# Show the plot.
plt.show()

# Exercise 4
# Call cross_val_score using logistic_regression and forest_classifier as models. Store the output as logistic_regression_scores and forest_classification_scores, respectively.
# Set the parameters cv=10 to use 10-fold cross-validation and scoring=correlation to use the accuracy function defined in Exercise 2.
logistic_regression_scores = cross_val_score(logistic_regression, covariates, classification_outcome, cv=10, scoring=accuracy)
forest_classification_scores = cross_val_score(forest_classifier, covariates, classification_outcome, cv=10, scoring=accuracy)

# Plotting code has been provided to compare the performance of the two models. Use plt.show() to plot the accuracy of predicted profitability for each cross-validation fold using the logistic and random forest classification models.
# Consider which of the two models exhibits a better fit.
plt.axes().set_aspect('equal', 'box')
plt.scatter(logistic_regression_scores, forest_classification_scores)
plt.plot((0, 1), (0, 1), 'k-')

plt.xlim(0, 1)
plt.ylim(0, 1)
plt.xlabel("Linear Classification Score")
plt.ylabel("Forest Classification Score")

# Exercise 5
# Define positive_revenue_df as the subset of movies in df with revenue greater than zero.
# Code is provided below that creates new instances of model objects. Replace all instances of df with positive_revenue_df, and run the given code.
positive_revenue_df = df[df.revenue > 0]

regression_outcome = positive_revenue_df[regression_target]
classification_outcome = positive_revenue_df[classification_target]
covariates = positive_revenue_df[all_covariates]

# Reinstantiate all regression models and classifiers.
linear_regression = LinearRegression()
logistic_regression = LogisticRegression()
forest_regression = RandomForestRegressor(max_depth=4, random_state=0)
forest_classifier = RandomForestClassifier(max_depth=4, random_state=0)
linear_regression_scores = cross_val_score(linear_regression, covariates, regression_outcome, cv=10, scoring=correlation)
forest_regression_scores = cross_val_score(forest_regression, covariates, regression_outcome, cv=10, scoring=correlation)
logistic_regression_scores = cross_val_score(logistic_regression, covariates, classification_outcome, cv=10, scoring=accuracy)
forest_classification_scores = cross_val_score(forest_classifier, covariates, classification_outcome, cv=10, scoring=accuracy)

# What is the mean of the 10 cross validation scores for random forest regression?
np.mean(forest_regression_scores)

# Exercise 6
# Call cross_val_score using logistic_regression and forest_classifier as models. Store the output as logistic_regression_scores and forest_classification_scores, respectively.
# Set the parameters cv=10 to use 10-fold cross-validation and scoring=correlation to use the accuracy function defined in Exercise 2.
# Plotting code has been provided to compare the performance of the two models. Use plt.show() to plot the correlation between actual and predicted revenue for each cross-validation fold using the logistic regression and random forest classification models. Consider which of the two models exhibits a better fit. Is this result different from what we observed when considering all movies?
plt.axes().set_aspect('equal', 'box')
plt.scatter(logistic_regression_scores, forest_classification_scores)
plt.plot((0, 1), (0, 1), 'k-')

plt.xlim(0, 1)
plt.ylim(0, 1)
plt.xlabel("Linear Classification Score")
plt.ylabel("Forest Classification Score")

plt.show()
# Code is provided for you that prints the importance of each covariate in predicting revenue using the random forest classifier. Consider which variables are the most important.
forest_classifier.fit(positive_revenue_df[all_covariates], classification_outcome)
sorted(list(zip(all_covariates, forest_classifier.feature_importances_)), key=lambda tup: tup[1])
